{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "PROJECT_REPO_DIR = \"/cluster/tufts/hugheslab/prath01/projects/time_series_prediction\"\n",
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_SRC_DIR = os.path.join(PROJECT_REPO_DIR, 'src')\n",
    "sys.path.append(PROJECT_SRC_DIR)\n",
    "from feature_transformation import (parse_id_cols, remove_col_names_from_list_if_not_in_df, parse_time_col, parse_feature_cols)\n",
    "from utils import load_data_dict_json\n",
    "\n",
    "sys.path.append(os.path.join(PROJECT_SRC_DIR, \"rnn\"))\n",
    "from dataset_loader import TidySequentialDataCSVLoader\n",
    "PROJECT_SRC_DIR = '/cluster/tufts/hugheslab/prath01/projects/time_series_prediction/src/'\n",
    "sys.path.append(PROJECT_SRC_DIR)\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "from feature_transformation import get_fenceposts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLF_TRAIN_TEST_SPLIT_PATH = '/cluster/tufts/hugheslab/prath01/projects/time_series_prediction/datasets/mimic-iv/v20220627/split-by=subject_id/sequence_features_custom_times/classifier_train_test_split_dir/'\n",
    "x_train_csv = os.path.join(CLF_TRAIN_TEST_SPLIT_PATH, 'x_trainCustomTimes_10_6_vitals_only.csv.gz')\n",
    "x_valid_csv = os.path.join(CLF_TRAIN_TEST_SPLIT_PATH, 'x_validCustomTimes_10_6_vitals_only.csv.gz')\n",
    "x_test_csv = os.path.join(CLF_TRAIN_TEST_SPLIT_PATH, 'x_testCustomTimes_10_6_vitals_only.csv.gz')\n",
    "\n",
    "\n",
    "y_train_csv = os.path.join(CLF_TRAIN_TEST_SPLIT_PATH, 'y_trainCustomTimes_10_6_vitals_only.csv.gz')\n",
    "y_valid_csv = os.path.join(CLF_TRAIN_TEST_SPLIT_PATH, 'y_validCustomTimes_10_6_vitals_only.csv.gz')\n",
    "y_test_csv = os.path.join(CLF_TRAIN_TEST_SPLIT_PATH, 'y_testCustomTimes_10_6_vitals_only.csv.gz')\n",
    "\n",
    "x_dict_json=os.path.join(CLF_TRAIN_TEST_SPLIT_PATH, 'x_dictCustomTimes_10_6_vitals_only.json')\n",
    "x_data_dict = load_data_dict_json(x_dict_json)\n",
    "\n",
    "x_train_df = pd.read_csv(x_train_csv)\n",
    "x_valid_df = pd.read_csv(x_valid_csv)\n",
    "x_test_df = pd.read_csv(x_test_csv)\n",
    "\n",
    "y_train_df = pd.read_csv(y_train_csv)\n",
    "y_valid_df = pd.read_csv(y_valid_csv)\n",
    "y_test_df = pd.read_csv(y_test_csv)\n",
    "\n",
    "# limit the EHR to first 48 hours\n",
    "max_t = 48\n",
    "x_train_df = x_train_df[x_train_df.stop<=max_t].reset_index(drop=True)\n",
    "y_train_df = y_train_df[y_train_df.stop<=max_t].reset_index(drop=True)\n",
    "\n",
    "x_valid_df = x_valid_df[x_valid_df.stop<=max_t].reset_index(drop=True)\n",
    "y_valid_df = y_valid_df[y_valid_df.stop<=max_t].reset_index(drop=True)\n",
    "\n",
    "x_test_df = x_test_df[x_test_df.stop<=max_t].reset_index(drop=True)\n",
    "y_test_df = y_test_df[y_test_df.stop<=max_t].reset_index(drop=True)\n",
    "\n",
    "y_train_df = y_train_df.drop_duplicates(subset='stay_id', keep='last').reset_index(drop=True)\n",
    "y_valid_df = y_valid_df.drop_duplicates(subset='stay_id', keep='last').reset_index(drop=True)\n",
    "y_test_df = y_test_df.drop_duplicates(subset='stay_id', keep='last').reset_index(drop=True)\n",
    "\n",
    "feature_cols = parse_feature_cols(x_data_dict['schema'])\n",
    "id_cols = parse_id_cols(x_data_dict['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================train===========================\n",
      "Number of slices in train : 42836\n",
      "Frac positive slices in train : 0.0136\n",
      "Number of admissions in train : 42836\n",
      "Frac positive admissions in train : 0.0136\n",
      "===================valid===========================\n",
      "Number of slices in valid : 15443\n",
      "Frac positive slices in valid : 0.0131\n",
      "Number of admissions in valid : 15443\n",
      "Frac positive admissions in valid : 0.0131\n",
      "===================test===========================\n",
      "Number of slices in test : 15802\n",
      "Frac positive slices in test : 0.0116\n",
      "Number of admissions in test : 15802\n",
      "Frac positive admissions in test : 0.0116\n"
     ]
    }
   ],
   "source": [
    "for split, y_df in [('train', y_train_df), \n",
    "                    ('valid', y_valid_df), \n",
    "                    ('test', y_test_df)]:\n",
    "    print('===================%s==========================='%split)\n",
    "    \n",
    "    n_adms_pos_outcome = len(y_df[y_df['in_icu_mortality']==1]['stay_id'].unique())\n",
    "    n_adms_total = len(y_df['stay_id'].unique())\n",
    "    \n",
    "    print('Number of slices in %s : %s'%(split, len(y_df)))\n",
    "    print('Frac positive slices in %s : %.4f'%(split, y_df['in_icu_mortality'].sum()/len(y_df)))\n",
    "    print('Number of admissions in %s : %s'%(split, n_adms_total))\n",
    "    print('Frac positive admissions in %s : %.4f'%(split, n_adms_pos_outcome/n_adms_total))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vitals = TidySequentialDataCSVLoader(\n",
    "    x_csv_path=x_train_df,\n",
    "    y_csv_path=y_train_df,\n",
    "    x_col_names=feature_cols,\n",
    "    idx_col_names=id_cols,\n",
    "    y_col_name=\"in_icu_mortality\",\n",
    "    y_label_type='per_sequence'\n",
    ")\n",
    "\n",
    "valid_vitals = TidySequentialDataCSVLoader(\n",
    "    x_csv_path=x_valid_df,\n",
    "    y_csv_path=y_valid_df,\n",
    "    x_col_names=feature_cols,\n",
    "    idx_col_names=id_cols,\n",
    "    y_col_name=\"in_icu_mortality\",\n",
    "    y_label_type='per_sequence'\n",
    ")\n",
    "\n",
    "test_vitals = TidySequentialDataCSVLoader(\n",
    "    x_csv_path=x_test_df,\n",
    "    y_csv_path=y_test_df,\n",
    "    x_col_names=feature_cols,\n",
    "    idx_col_names=id_cols,\n",
    "    y_col_name=\"in_icu_mortality\",\n",
    "    y_label_type='per_sequence'\n",
    ")\n",
    "\n",
    "# num_true_feats = int(F/3)\n",
    "train_x_NTD, y_train = train_vitals.get_batch_data(batch_id=0)\n",
    "valid_x_NTD, y_valid = valid_vitals.get_batch_data(batch_id=0)\n",
    "test_x_NTD, y_test = test_vitals.get_batch_data(batch_id=0)\n",
    "\n",
    "N_tr = len(train_x_NTD)\n",
    "N_va = len(valid_x_NTD)\n",
    "N_te = len(test_x_NTD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42836,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 1.200 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Excluded inds train: 3658, 24411, 36568 ... 21793, 12641, 38467\n",
      "Excluded inds valid: 14685, 4004, 13801 ... 14248, 4256, 6648\n",
      "Excluded inds test: 10851, 4531, 10134 ... 6648, 14074, 5532\n",
      "---------------------------------------------------------------------------\n",
      "fraction positive labels in train set with 1.200 percent of sequences labelled : 0.0078\n",
      "fraction positive labels in valid set with 1.200 percent of sequences labelled : 0.0054\n",
      "fraction positive labels in test set with 1.200 percent of sequences labelled : 0.0053\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 3.700 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Excluded inds train: 3658, 24411, 36568 ... 12705, 41004, 20243\n",
      "Excluded inds valid: 14685, 4004, 13801 ... 8849, 9201, 5668\n",
      "Excluded inds test: 10851, 4531, 10134 ... 11803, 992, 8849\n",
      "---------------------------------------------------------------------------\n",
      "fraction positive labels in train set with 3.700 percent of sequences labelled : 0.0107\n",
      "fraction positive labels in valid set with 3.700 percent of sequences labelled : 0.0122\n",
      "fraction positive labels in test set with 3.700 percent of sequences labelled : 0.0103\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 11.100 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Excluded inds train: 3658, 24411, 36568 ... 6336, 35383, 34955\n",
      "Excluded inds valid: 14685, 4004, 13801 ... 15352, 725, 1279\n",
      "Excluded inds test: 10851, 4531, 10134 ... 405, 10377, 14708\n",
      "---------------------------------------------------------------------------\n",
      "fraction positive labels in train set with 11.100 percent of sequences labelled : 0.0122\n",
      "fraction positive labels in valid set with 11.100 percent of sequences labelled : 0.0105\n",
      "fraction positive labels in test set with 11.100 percent of sequences labelled : 0.0114\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 33.300 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Excluded inds train: 3658, 24411, 36568 ... 2042, 2690, 18004\n",
      "Excluded inds valid: 14685, 4004, 13801 ... 2715, 8526, 12908\n",
      "Excluded inds test: 10851, 4531, 10134 ... 8526, 11212, 844\n",
      "---------------------------------------------------------------------------\n",
      "fraction positive labels in train set with 33.300 percent of sequences labelled : 0.0129\n",
      "fraction positive labels in valid set with 33.300 percent of sequences labelled : 0.0130\n",
      "fraction positive labels in test set with 33.300 percent of sequences labelled : 0.0112\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 100.000 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "fraction positive labels in train set with 100.000 percent of sequences labelled : 0.0136\n",
      "fraction positive labels in valid set with 100.000 percent of sequences labelled : 0.0131\n",
      "fraction positive labels in test set with 100.000 percent of sequences labelled : 0.0116\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "state_id = 41\n",
    "data_save_dir = '/cluster/tufts/hugheslab/prath01/datasets/mimic4_ssl/'\n",
    "\n",
    "for ii, perc_labelled in enumerate([1.2, 3.7, 11.1, 33.3, 100]):#3.7, 11.1, 33.3, 100\n",
    "    curr_save_dir = os.path.join(data_save_dir, 'percentage_labelled_sequnces=%s'%perc_labelled)\n",
    "    \n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('CREATING TRAIN/VALID/TEST SPLITS FOR %.3f PERCENT OF SEQUENCES LABELLED'%perc_labelled)\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    y_train_ss = y_train.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_tr = int((1-(perc_labelled)/100)*N_tr)\n",
    "    unlabelled_inds_tr = rnd_state.permutation(N_tr)[:n_unlabelled_tr]\n",
    "    y_train_ss = y_train_ss.astype(np.float32)\n",
    "    y_train_ss[unlabelled_inds_tr] = np.nan  \n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds train: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_tr[0],\n",
    "                                                              unlabelled_inds_tr[1],\n",
    "                                                              unlabelled_inds_tr[2],\n",
    "                                                              unlabelled_inds_tr[-3],\n",
    "                                                              unlabelled_inds_tr[-2],\n",
    "                                                              unlabelled_inds_tr[-1]))\n",
    "    \n",
    "    y_valid_ss = y_valid.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_va = int((1-(perc_labelled)/100)*N_va)\n",
    "    unlabelled_inds_va = rnd_state.permutation(N_va)[:n_unlabelled_va]\n",
    "    y_valid_ss = y_valid_ss.astype(np.float32)\n",
    "    y_valid_ss[unlabelled_inds_va] = np.nan \n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds valid: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_va[0],\n",
    "                                                          unlabelled_inds_va[1],\n",
    "                                                          unlabelled_inds_va[2],\n",
    "                                                          unlabelled_inds_va[-3],\n",
    "                                                          unlabelled_inds_va[-2],\n",
    "                                                          unlabelled_inds_va[-1]))\n",
    "\n",
    "    y_test_ss = y_test.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_te = int((1-(perc_labelled)/100)*N_te)\n",
    "    unlabelled_inds_te = rnd_state.permutation(N_te)[:n_unlabelled_te]\n",
    "    y_test_ss = y_test_ss.astype(np.float32)\n",
    "    y_test_ss[unlabelled_inds_te] = np.nan\n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds test: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_te[0],\n",
    "                                                          unlabelled_inds_te[1],\n",
    "                                                          unlabelled_inds_te[2],\n",
    "                                                          unlabelled_inds_te[-3],\n",
    "                                                          unlabelled_inds_te[-2],\n",
    "                                                          unlabelled_inds_te[-1]))\n",
    "    \n",
    "#     # Check whether the specified path exists or not\n",
    "#     isExist = os.path.exists(curr_save_dir)\n",
    "\n",
    "#     if not isExist:\n",
    "#         # Create a new directory because it does not exist \n",
    "#         os.makedirs(curr_save_dir)\n",
    "        \n",
    "#     # save the data to the respective folder\n",
    "#     print('Saving data to %s'%curr_save_dir)\n",
    "#     np.save(os.path.join(curr_save_dir, 'X_train.npy'), train_x_NTD)\n",
    "#     np.save(os.path.join(curr_save_dir, 'y_train.npy'), y_train_ss)\n",
    "#     print('Done saving train..')\n",
    "#     np.save(os.path.join(curr_save_dir, 'X_valid.npy'), valid_x_NTD)\n",
    "#     np.save(os.path.join(curr_save_dir, 'y_valid.npy'), y_valid_ss)\n",
    "#     print('Done saving valid..')\n",
    "#     np.save(os.path.join(curr_save_dir, 'X_test.npy'), test_x_NTD)\n",
    "#     np.save(os.path.join(curr_save_dir, 'y_test.npy'), y_test_ss)\n",
    "#     print('Done saving test..')\n",
    "    \n",
    "    \n",
    "    print('---------------------------------------------------------------------------')\n",
    "    for split, y in [('train', y_train_ss),\n",
    "                    ('valid', y_valid_ss),\n",
    "                    ('test', y_test_ss)]:\n",
    "        frac_pos_labels = np.nansum(y)/(~np.isnan(y)).sum()\n",
    "        print('fraction positive labels in %s set with %.3f percent of sequences labelled : %.4f'%(split,\n",
    "                                                                                                   perc_labelled,\n",
    "                                                                                                   frac_pos_labels))\n",
    "    print('---------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(.7+.5+.5)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make collapsed version of dataset for LR, RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featurize_single_time_series import collapse_std, collapse_elapsed_time_since_last_measured, collapse_count, collapse_slope, collapse_median, collapse_min, collapse_max, collapse_value_last_measured, make_summary_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse features\n",
    "def featurize_ts(\n",
    "        time_arr_by_var,\n",
    "        val_arr_by_var,\n",
    "        n_features,\n",
    "        percentile_slices_to_featurize=[(0., 100.)],\n",
    "        summary_ops=['count', 'mean', 'std', 'slope'],\n",
    "        ):\n",
    "    ''' Featurize provided multivariate irregular time series into flat vector\n",
    "    Args\n",
    "    ----\n",
    "    time_arr_by_var : dict of 1D NumPy arrays\n",
    "    val_arr_by_var : dict of 1D NumPy arrays\n",
    "    start_numerictime : float\n",
    "        Indicates numerical time value at which current window *starts*\n",
    "    stop_numerictime : float\n",
    "        Indicates numerical time that current window *stops*\n",
    "    Returns\n",
    "    -------\n",
    "    feat_vec_1F : 2D NumPy array, shape (1, F)\n",
    "        One entry for each combination of {variable, summary op, subwindow slice}\n",
    "    '''\n",
    "    \n",
    "    start_numerictime = 0\n",
    "    stop_numerictime = 24\n",
    "    time_range = stop_numerictime - start_numerictime\n",
    "\n",
    "    F = len(percentile_slices_to_featurize) * n_features * len (summary_ops)\n",
    "    feat_vec_1F = np.zeros((1, F))\n",
    "    ff = 0\n",
    "\n",
    "    SUMMARY_OPERATIONS = make_summary_ops()\n",
    "\n",
    "    for rp_ind, (low, high) in enumerate(percentile_slices_to_featurize):\n",
    "        cur_window_start_time = start_numerictime + float(low) / 100 * time_range\n",
    "        cur_window_stop_time = start_numerictime + float(high) / 100 * time_range\n",
    "\n",
    "        for var_id in range(n_features):\n",
    "            cur_feat_arr = val_arr_by_var[:, var_id].astype('float')\n",
    "            cur_numerictime_arr = time_arr_by_var\n",
    "\n",
    "            # Keep only the entries whose times occur within current window\n",
    "            start = np.searchsorted(\n",
    "                cur_numerictime_arr, cur_window_start_time, side='left')\n",
    "            stop = np.searchsorted(\n",
    "                cur_numerictime_arr, cur_window_stop_time, side='right')\n",
    "            cur_numerictime_arr = cur_numerictime_arr[start:stop]\n",
    "            cur_feat_arr = cur_feat_arr[start:stop]\n",
    "            cur_isfinite_arr = np.isfinite(cur_feat_arr)\n",
    "            \n",
    "            for op_ind, op in enumerate(summary_ops):\n",
    "                summary_func, empty_val = SUMMARY_OPERATIONS[op]\n",
    "                if cur_feat_arr.size < 1 or cur_isfinite_arr.sum() < 1:\n",
    "                    feat_vec_1F[0,ff] = empty_val\n",
    "                else:\n",
    "                    feat_vec_1F[0,ff] = summary_func(\n",
    "                        cur_feat_arr, cur_numerictime_arr, cur_isfinite_arr,\n",
    "                        cur_window_start_time, cur_window_stop_time)\n",
    "#                 feat_names.append(\"feature_%s_%s_%.0f-%.0f\" % (var_id, op, float(low), float(high)))\n",
    "                ff += 1\n",
    "    return feat_vec_1F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing train feaures\n",
      "Done with 0 sequences..\n",
      "Done with 500 sequences..\n",
      "Done with 1000 sequences..\n",
      "Done with 1500 sequences..\n",
      "Done with 2000 sequences..\n",
      "Done with 2500 sequences..\n",
      "Done with 3000 sequences..\n",
      "Done with 3500 sequences..\n",
      "Done with 4000 sequences..\n",
      "Done with 4500 sequences..\n",
      "Done with 5000 sequences..\n",
      "Done with 5500 sequences..\n",
      "Done with 6000 sequences..\n",
      "Done with 6500 sequences..\n",
      "Done with 7000 sequences..\n",
      "Done with 7500 sequences..\n",
      "Done with 8000 sequences..\n",
      "Done with 8500 sequences..\n",
      "Done with 9000 sequences..\n",
      "Done with 9500 sequences..\n",
      "Done with 10000 sequences..\n",
      "Done with 10500 sequences..\n",
      "Done with 11000 sequences..\n",
      "Done with 11500 sequences..\n",
      "Done with 12000 sequences..\n",
      "Done with 12500 sequences..\n",
      "Done with 13000 sequences..\n",
      "Done with 13500 sequences..\n",
      "Done with 14000 sequences..\n",
      "Done with 14500 sequences..\n",
      "Done with 15000 sequences..\n",
      "Done with 15500 sequences..\n",
      "Done with 16000 sequences..\n",
      "Done with 16500 sequences..\n",
      "Done with 17000 sequences..\n",
      "Done with 17500 sequences..\n",
      "Done with 18000 sequences..\n",
      "Done with 18500 sequences..\n",
      "Done with 19000 sequences..\n",
      "Done with 19500 sequences..\n",
      "Done with 20000 sequences..\n",
      "Done with 20500 sequences..\n",
      "Done with 21000 sequences..\n",
      "Done with 21500 sequences..\n",
      "Done with 22000 sequences..\n",
      "Done with 22500 sequences..\n",
      "Done with 23000 sequences..\n",
      "Done with 23500 sequences..\n",
      "Done with 24000 sequences..\n",
      "Done with 24500 sequences..\n",
      "Done with 25000 sequences..\n",
      "Done with 25500 sequences..\n",
      "Done with 26000 sequences..\n",
      "Done with 26500 sequences..\n",
      "Done with 27000 sequences..\n",
      "Done with 27500 sequences..\n",
      "Done with 28000 sequences..\n",
      "Done with 28500 sequences..\n",
      "Done with 29000 sequences..\n",
      "Done with 29500 sequences..\n",
      "Done with 30000 sequences..\n",
      "Done with 30500 sequences..\n",
      "Done with 31000 sequences..\n",
      "Done with 31500 sequences..\n",
      "Done with 32000 sequences..\n",
      "Done with 32500 sequences..\n",
      "Done with 33000 sequences..\n",
      "Done with 33500 sequences..\n",
      "Done with 34000 sequences..\n",
      "Done with 34500 sequences..\n",
      "Done with 35000 sequences..\n",
      "Done with 35500 sequences..\n",
      "Done with 36000 sequences..\n",
      "Done with 36500 sequences..\n",
      "Done with 37000 sequences..\n",
      "Done with 37500 sequences..\n",
      "Done with 38000 sequences..\n",
      "Done with 38500 sequences..\n",
      "Done with 39000 sequences..\n",
      "Done with 39500 sequences..\n",
      "Done with 40000 sequences..\n",
      "Done with 40500 sequences..\n",
      "Done with 41000 sequences..\n",
      "Done with 41500 sequences..\n",
      "Done with 42000 sequences..\n",
      "Done with 42500 sequences..\n",
      "Collapsing valid feaures\n",
      "Done with 0 sequences..\n",
      "Done with 500 sequences..\n",
      "Done with 1000 sequences..\n",
      "Done with 1500 sequences..\n",
      "Done with 2000 sequences..\n",
      "Done with 2500 sequences..\n",
      "Done with 3000 sequences..\n",
      "Done with 3500 sequences..\n",
      "Done with 4000 sequences..\n",
      "Done with 4500 sequences..\n",
      "Done with 5000 sequences..\n",
      "Done with 5500 sequences..\n",
      "Done with 6000 sequences..\n",
      "Done with 6500 sequences..\n",
      "Done with 7000 sequences..\n",
      "Done with 7500 sequences..\n",
      "Done with 8000 sequences..\n",
      "Done with 8500 sequences..\n",
      "Done with 9000 sequences..\n",
      "Done with 9500 sequences..\n",
      "Done with 10000 sequences..\n",
      "Done with 10500 sequences..\n",
      "Done with 11000 sequences..\n",
      "Done with 11500 sequences..\n",
      "Done with 12000 sequences..\n",
      "Done with 12500 sequences..\n",
      "Done with 13000 sequences..\n",
      "Done with 13500 sequences..\n",
      "Done with 14000 sequences..\n",
      "Done with 14500 sequences..\n",
      "Done with 15000 sequences..\n",
      "Collapsing test feaures\n",
      "Done with 0 sequences..\n",
      "Done with 500 sequences..\n",
      "Done with 1000 sequences..\n",
      "Done with 1500 sequences..\n",
      "Done with 2000 sequences..\n",
      "Done with 2500 sequences..\n",
      "Done with 3000 sequences..\n",
      "Done with 3500 sequences..\n",
      "Done with 4000 sequences..\n",
      "Done with 4500 sequences..\n",
      "Done with 5000 sequences..\n",
      "Done with 5500 sequences..\n",
      "Done with 6000 sequences..\n",
      "Done with 6500 sequences..\n",
      "Done with 7000 sequences..\n",
      "Done with 7500 sequences..\n",
      "Done with 8000 sequences..\n",
      "Done with 8500 sequences..\n",
      "Done with 9000 sequences..\n",
      "Done with 9500 sequences..\n",
      "Done with 10000 sequences..\n",
      "Done with 10500 sequences..\n",
      "Done with 11000 sequences..\n",
      "Done with 11500 sequences..\n",
      "Done with 12000 sequences..\n",
      "Done with 12500 sequences..\n",
      "Done with 13000 sequences..\n",
      "Done with 13500 sequences..\n",
      "Done with 14000 sequences..\n",
      "Done with 14500 sequences..\n",
      "Done with 15000 sequences..\n",
      "Done with 15500 sequences..\n"
     ]
    }
   ],
   "source": [
    "N_tr = len(train_x_NTD)\n",
    "N_va = len(valid_x_NTD)\n",
    "N_te = len(test_x_NTD)\n",
    "percentile_slices_to_featurize = [(0., 100.)]\n",
    "summary_ops = [\"std\", \"time_since_measured\", \"count\", \"slope\", \"median\", \"min\", \"max\"]\n",
    "n_features = train_x_NTD.shape[-1]\n",
    "F = len(percentile_slices_to_featurize) * n_features * len (summary_ops)\n",
    "fps_train = get_fenceposts(x_train_df, id_cols)\n",
    "fps_valid = get_fenceposts(x_valid_df, id_cols)\n",
    "fps_test = get_fenceposts(x_test_df, id_cols)\n",
    "\n",
    "\n",
    "train_x_collapsed_NF = np.zeros((N_tr, F))\n",
    "valid_x_collapsed_NF = np.zeros((N_va, F))\n",
    "test_x_collapsed_NF = np.zeros((N_te, F))\n",
    "\n",
    "print('Collapsing train feaures')\n",
    "for nn in range(N_tr):\n",
    "    if (nn%500)==0:\n",
    "        print('Done with %s sequences..'%nn)\n",
    "    \n",
    "    T = train_x_NTD.shape[1]\n",
    "    train_x_collapsed_NF[nn, :] = featurize_ts(np.arange(0, T).astype(float),\n",
    "                                               train_x_NTD[nn],\n",
    "                                               n_features,\n",
    "                                               percentile_slices_to_featurize=percentile_slices_to_featurize,\n",
    "                                               summary_ops=summary_ops)\n",
    "print('Collapsing valid feaures')\n",
    "for nn in range(N_va):\n",
    "    if (nn%500)==0:\n",
    "        print('Done with %s sequences..'%nn)\n",
    "    valid_x_collapsed_NF[nn, :] = featurize_ts(np.arange(0, T).astype(float),\n",
    "                                               valid_x_NTD[nn],\n",
    "                                               n_features,\n",
    "                                               percentile_slices_to_featurize=percentile_slices_to_featurize,\n",
    "                                               summary_ops=summary_ops)\n",
    "\n",
    "print('Collapsing test feaures')\n",
    "for nn in range(N_te):\n",
    "    if (nn%500)==0:\n",
    "        print('Done with %s sequences..'%nn)\n",
    "    test_x_collapsed_NF[nn, :] = featurize_ts(np.arange(0, T).astype(float),\n",
    "                                               test_x_NTD[nn],\n",
    "                                               n_features,\n",
    "                                               percentile_slices_to_featurize=percentile_slices_to_featurize,\n",
    "                                               summary_ops=summary_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 1.200 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Excluded inds train: 3658, 24411, 36568 ... 21793, 12641, 38467\n",
      "Excluded inds valid: 14685, 4004, 13801 ... 14248, 4256, 6648\n",
      "Excluded inds test: 10851, 4531, 10134 ... 6648, 14074, 5532\n",
      "Saving data to /cluster/tufts/hugheslab/prath01/datasets/mimic4_ssl/percentage_labelled_sequnces=1.2\n",
      "Done saving train..\n",
      "Done saving valid..\n",
      "Done saving test..\n",
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 3.700 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Excluded inds train: 3658, 24411, 36568 ... 12705, 41004, 20243\n",
      "Excluded inds valid: 14685, 4004, 13801 ... 8849, 9201, 5668\n",
      "Excluded inds test: 10851, 4531, 10134 ... 11803, 992, 8849\n",
      "Saving data to /cluster/tufts/hugheslab/prath01/datasets/mimic4_ssl/percentage_labelled_sequnces=3.7\n",
      "Done saving train..\n",
      "Done saving valid..\n",
      "Done saving test..\n",
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 11.100 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Excluded inds train: 3658, 24411, 36568 ... 6336, 35383, 34955\n",
      "Excluded inds valid: 14685, 4004, 13801 ... 15352, 725, 1279\n",
      "Excluded inds test: 10851, 4531, 10134 ... 405, 10377, 14708\n",
      "Saving data to /cluster/tufts/hugheslab/prath01/datasets/mimic4_ssl/percentage_labelled_sequnces=11.1\n",
      "Done saving train..\n",
      "Done saving valid..\n",
      "Done saving test..\n",
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 33.300 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Excluded inds train: 3658, 24411, 36568 ... 2042, 2690, 18004\n",
      "Excluded inds valid: 14685, 4004, 13801 ... 2715, 8526, 12908\n",
      "Excluded inds test: 10851, 4531, 10134 ... 8526, 11212, 844\n",
      "Saving data to /cluster/tufts/hugheslab/prath01/datasets/mimic4_ssl/percentage_labelled_sequnces=33.3\n",
      "Done saving train..\n",
      "Done saving valid..\n",
      "Done saving test..\n",
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 100.000 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Saving data to /cluster/tufts/hugheslab/prath01/datasets/mimic4_ssl/percentage_labelled_sequnces=100\n",
      "Done saving train..\n",
      "Done saving valid..\n",
      "Done saving test..\n"
     ]
    }
   ],
   "source": [
    "state_id = 41\n",
    "data_save_dir = '/cluster/tufts/hugheslab/prath01/datasets/mimic4_ssl/'\n",
    "\n",
    "for ii, perc_labelled in enumerate([1.2, 3.7, 11.1, 33.3, 100]):#3.7, 11.1, 33.3, 100\n",
    "    curr_save_dir = os.path.join(data_save_dir, 'percentage_labelled_sequnces=%s'%perc_labelled)\n",
    "    \n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('CREATING TRAIN/VALID/TEST SPLITS FOR %.3f PERCENT OF SEQUENCES LABELLED'%perc_labelled)\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    y_train_ss = y_train.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_tr = int((1-(perc_labelled)/100)*N_tr)\n",
    "    unlabelled_inds_tr = rnd_state.permutation(N_tr)[:n_unlabelled_tr]\n",
    "    y_train_ss = y_train_ss.astype(np.float32)\n",
    "    y_train_ss[unlabelled_inds_tr] = np.nan  \n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds train: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_tr[0],\n",
    "                                                              unlabelled_inds_tr[1],\n",
    "                                                              unlabelled_inds_tr[2],\n",
    "                                                              unlabelled_inds_tr[-3],\n",
    "                                                              unlabelled_inds_tr[-2],\n",
    "                                                              unlabelled_inds_tr[-1]))\n",
    "    \n",
    "    y_valid_ss = y_valid.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_va = int((1-(perc_labelled)/100)*N_va)\n",
    "    unlabelled_inds_va = rnd_state.permutation(N_va)[:n_unlabelled_va]\n",
    "    y_valid_ss = y_valid_ss.astype(np.float32)\n",
    "    y_valid_ss[unlabelled_inds_va] = np.nan \n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds valid: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_va[0],\n",
    "                                                          unlabelled_inds_va[1],\n",
    "                                                          unlabelled_inds_va[2],\n",
    "                                                          unlabelled_inds_va[-3],\n",
    "                                                          unlabelled_inds_va[-2],\n",
    "                                                          unlabelled_inds_va[-1]))\n",
    "\n",
    "    y_test_ss = y_test.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_te = int((1-(perc_labelled)/100)*N_te)\n",
    "    unlabelled_inds_te = rnd_state.permutation(N_te)[:n_unlabelled_te]\n",
    "    y_test_ss = y_test_ss.astype(np.float32)\n",
    "    y_test_ss[unlabelled_inds_te] = np.nan\n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds test: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_te[0],\n",
    "                                                          unlabelled_inds_te[1],\n",
    "                                                          unlabelled_inds_te[2],\n",
    "                                                          unlabelled_inds_te[-3],\n",
    "                                                          unlabelled_inds_te[-2],\n",
    "                                                          unlabelled_inds_te[-1]))\n",
    "    \n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(curr_save_dir)\n",
    "\n",
    "    if not isExist:\n",
    "        # Create a new directory because it does not exist \n",
    "        os.makedirs(curr_save_dir)\n",
    "        \n",
    "    # save the data to the respective folder\n",
    "    print('Saving data to %s'%curr_save_dir)\n",
    "    np.save(os.path.join(curr_save_dir, 'X_train_collapsed.npy'), train_x_collapsed_NF)\n",
    "    np.save(os.path.join(curr_save_dir, 'y_train_collapsed.npy'), y_train_ss)\n",
    "    print('Done saving train..')\n",
    "    np.save(os.path.join(curr_save_dir, 'X_valid_collapsed.npy'), valid_x_collapsed_NF)\n",
    "    np.save(os.path.join(curr_save_dir, 'y_valid_collapsed.npy'), y_valid_ss)\n",
    "    print('Done saving valid..')\n",
    "    np.save(os.path.join(curr_save_dir, 'X_test_collapsed.npy'), test_x_collapsed_NF)\n",
    "    np.save(os.path.join(curr_save_dir, 'y_test_collapsed.npy'), y_test_ss)\n",
    "    print('Done saving test..')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42836"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>blood_glucose_concentration</th>\n",
       "      <th>bmi</th>\n",
       "      <th>body_temperature</th>\n",
       "      <th>diastolic_blood_pressure</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>height</th>\n",
       "      <th>o2_sat</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>systolic_blood_pressure</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_timestamp</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>Age</th>\n",
       "      <th>is_gender_male</th>\n",
       "      <th>is_gender_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10001217</td>\n",
       "      <td>24597018</td>\n",
       "      <td>37067082</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>-17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2157-11-20 19:18:02</td>\n",
       "      <td>2:00:00</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10001217</td>\n",
       "      <td>24597018</td>\n",
       "      <td>37067082</td>\n",
       "      <td>-17.3</td>\n",
       "      <td>-9.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2157-11-20 19:18:02</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10001217</td>\n",
       "      <td>24597018</td>\n",
       "      <td>37067082</td>\n",
       "      <td>-9.3</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2157-11-20 19:18:02</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10001217</td>\n",
       "      <td>24597018</td>\n",
       "      <td>37067082</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.138890</td>\n",
       "      <td>90.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2157-11-20 19:18:02</td>\n",
       "      <td>2:00:00</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10001217</td>\n",
       "      <td>24597018</td>\n",
       "      <td>37067082</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.722220</td>\n",
       "      <td>72.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2157-11-20 19:18:02</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326904</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>36195440</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.222220</td>\n",
       "      <td>67.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2145-11-02 22:59:00</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326905</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>36195440</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.166668</td>\n",
       "      <td>68.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2145-11-02 22:59:00</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326906</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>36195440</td>\n",
       "      <td>19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.166668</td>\n",
       "      <td>59.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2145-11-02 22:59:00</td>\n",
       "      <td>2:00:00</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326907</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>36195440</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.111110</td>\n",
       "      <td>72.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2145-11-02 22:59:00</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326908</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>36195440</td>\n",
       "      <td>35.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.722220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2145-11-02 22:59:00</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326909 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subject_id   hadm_id   stay_id  start  stop  \\\n",
       "0         10001217  24597018  37067082  -24.0 -17.3   \n",
       "1         10001217  24597018  37067082  -17.3  -9.3   \n",
       "2         10001217  24597018  37067082   -9.3  -1.3   \n",
       "3         10001217  24597018  37067082   -1.3   6.7   \n",
       "4         10001217  24597018  37067082    6.7  14.7   \n",
       "...            ...       ...       ...    ...   ...   \n",
       "326904    19999987  23865745  36195440    3.0  11.0   \n",
       "326905    19999987  23865745  36195440   11.0  19.0   \n",
       "326906    19999987  23865745  36195440   19.0  27.0   \n",
       "326907    19999987  23865745  36195440   27.0  35.0   \n",
       "326908    19999987  23865745  36195440   35.0  43.0   \n",
       "\n",
       "        blood_glucose_concentration  bmi  body_temperature  \\\n",
       "0                               NaN  NaN               NaN   \n",
       "1                               NaN  NaN               NaN   \n",
       "2                               NaN  NaN               NaN   \n",
       "3                               NaN  NaN         37.138890   \n",
       "4                               NaN  NaN         36.722220   \n",
       "...                             ...  ...               ...   \n",
       "326904                          NaN  NaN         37.222220   \n",
       "326905                          NaN  NaN         37.166668   \n",
       "326906                          NaN  NaN         37.166668   \n",
       "326907                          NaN  NaN         38.111110   \n",
       "326908                          NaN  NaN         37.722220   \n",
       "\n",
       "        diastolic_blood_pressure  heart_rate  height  o2_sat  \\\n",
       "0                            NaN         NaN     NaN     NaN   \n",
       "1                            NaN         NaN     NaN     NaN   \n",
       "2                            NaN         NaN     NaN     NaN   \n",
       "3                           90.0        86.0     NaN    99.0   \n",
       "4                           72.0        89.0     NaN    98.0   \n",
       "...                          ...         ...     ...     ...   \n",
       "326904                      67.0        90.0     NaN   100.0   \n",
       "326905                      68.0       103.0     NaN    93.0   \n",
       "326906                      59.0       102.0     NaN   100.0   \n",
       "326907                      72.0       113.0     NaN    99.0   \n",
       "326908                       NaN        95.0     NaN     NaN   \n",
       "\n",
       "        respiratory_rate  systolic_blood_pressure  weight  \\\n",
       "0                    NaN                      NaN     NaN   \n",
       "1                    NaN                      NaN     NaN   \n",
       "2                    NaN                      NaN     NaN   \n",
       "3                   18.0                    151.0     NaN   \n",
       "4                   19.0                    141.0     NaN   \n",
       "...                  ...                      ...     ...   \n",
       "326904              20.0                    118.0     NaN   \n",
       "326905              19.0                    111.0     NaN   \n",
       "326906              22.0                    101.0     NaN   \n",
       "326907              27.0                    116.0     NaN   \n",
       "326908              27.0                      NaN     NaN   \n",
       "\n",
       "        admission_timestamp stop_time  Age  is_gender_male  is_gender_unknown  \n",
       "0       2157-11-20 19:18:02   2:00:00   55               0                  0  \n",
       "1       2157-11-20 19:18:02  10:00:00   55               0                  0  \n",
       "2       2157-11-20 19:18:02  18:00:00   55               0                  0  \n",
       "3       2157-11-20 19:18:02   2:00:00   55               0                  0  \n",
       "4       2157-11-20 19:18:02  10:00:00   55               0                  0  \n",
       "...                     ...       ...  ...             ...                ...  \n",
       "326904  2145-11-02 22:59:00  10:00:00   57               0                  0  \n",
       "326905  2145-11-02 22:59:00  18:00:00   57               0                  0  \n",
       "326906  2145-11-02 22:59:00   2:00:00   57               0                  0  \n",
       "326907  2145-11-02 22:59:00  10:00:00   57               0                  0  \n",
       "326908  2145-11-02 22:59:00  18:00:00   57               0                  0  \n",
       "\n",
       "[326909 rows x 20 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
